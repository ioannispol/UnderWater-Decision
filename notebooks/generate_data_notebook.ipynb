{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load the attached Excel file to use as a reference for synthesizing the dataset\n",
    "excel_file_path = '/workspaces/UnderWater-Decision/data/mf-data.xlsx'\n",
    "xls = pd.ExcelFile(excel_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract platform names from sheet names\n",
    "platform_names = xls.sheet_names\n",
    "# Remove non-platform sheets from the list\n",
    "non_platform_sheets = ['Explanation', 'Location', 'All', 'Sheet9']\n",
    "platforms = [name for name in platform_names if name not in non_platform_sheets]\n",
    "\n",
    "structures = platforms\n",
    "locations = ['North Sea']  # Assuming all structures are in the North Sea\n",
    "cleaning_methods = ['Method A', 'Method B', 'Method C']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to generate a random date for last cleaning\n",
    "def generate_random_date(start_year=2015, end_year=2021):\n",
    "    start_date = datetime(year=start_year, month=1, day=1)\n",
    "    end_date = datetime(year=end_year, month=1, day=1)\n",
    "    return (start_date + timedelta(days=random.randint(0, (end_date - start_date).days))).date()\n",
    "\n",
    "# Function to generate synthetic dataset\n",
    "def generate_synthetic_data(num_entries):\n",
    "    synthetic_data = []\n",
    "\n",
    "    for _ in range(num_entries):\n",
    "        structure_id = f\"{random.randint(1, 999):03}\"\n",
    "        structure_type = random.choice(structures)\n",
    "        location = random.choice(locations)\n",
    "        age = random.randint(1, 20)  # Structures aged between 1 to 20 years\n",
    "        last_cleaning = generate_random_date()\n",
    "        image_path = f\"/path/img{structure_id}\"\n",
    "        detected_algae = random.randint(0, 60)  # Algae coverage between 0% to 60%\n",
    "        detected_barnacles = random.randint(0, 40)  # Barnacle coverage between 0% to 40%\n",
    "        detected_mussels = random.randint(0, 30)  # Mussel coverage between 0% to 30%\n",
    "        total_coverage = detected_algae + detected_barnacles + detected_mussels\n",
    "        recommended_cleaning_method = random.choice(cleaning_methods)\n",
    "\n",
    "        # Ensure total coverage doesn't exceed 100%\n",
    "        total_coverage = min(total_coverage, 100)\n",
    "\n",
    "        synthetic_data.append({\n",
    "            \"Structure_ID\": structure_id,\n",
    "            \"Structure_Type\": structure_type,\n",
    "            \"Location\": location,\n",
    "            \"Age (years)\": age,\n",
    "            \"Last_Cleaning\": last_cleaning,\n",
    "            \"Image_Path\": image_path,\n",
    "            \"Detected_Algae (%)\": detected_algae,\n",
    "            \"Detected_Barnacles (%)\": detected_barnacles,\n",
    "            \"Detected_Mussels (%)\": detected_mussels,\n",
    "            \"Total_Coverage (%)\": total_coverage,\n",
    "            \"Recommended_Cleaning_Method\": recommended_cleaning_method\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(synthetic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/UnderWater-Decision/data/synthetic_dataset1.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Generate a synthetic dataset with the desired number of entries\n",
    "synthetic_dataset = generate_synthetic_data(1000)\n",
    "synthetic_dataset_path = '/workspaces/UnderWater-Decision/data/synthetic_dataset1.csv'\n",
    "synthetic_dataset.to_csv(synthetic_dataset_path, index=False)\n",
    "\n",
    "# Output the path to the generated CSV file\n",
    "synthetic_dataset_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define the parameters for the synthetic dataset based on the provided data structure\n",
    "structures = ['Oil Rig', 'Wind Turbine', 'Underwater Pipeline', 'Ship Hull']\n",
    "locations = ['North Sea', 'Gulf of Mexico', 'Baltic Sea', 'Pacific Ocean']\n",
    "cleaning_methods = ['Method A', 'Method B', 'Method C']\n",
    "items = ['Level 1', 'Risers', 'Caissons', 'Conductors']\n",
    "years = [2010, 2012, 2013, 2015]\n",
    "\n",
    "# Define a function to generate a random date for last cleaning\n",
    "def generate_random_date(start_year=2015, end_year=2021):\n",
    "    start_date = datetime(year=start_year, month=1, day=1)\n",
    "    end_date = datetime(year=end_year, month=1, day=1)\n",
    "    return (start_date + timedelta(days=random.randint(0, (end_date - start_date).days))).date()\n",
    "\n",
    "# Function to generate synthetic dataset\n",
    "def generate_synthetic_data(num_entries):\n",
    "    synthetic_data = []\n",
    "\n",
    "    for _ in range(num_entries):\n",
    "        structure_id = f\"{random.randint(1, 999):03}\"\n",
    "        structure_type = random.choice(structures)\n",
    "        location = random.choice(locations)\n",
    "        year = random.choice(years)\n",
    "        depthmin = random.uniform(-50, 0)  # Assuming depthmin is a negative value below water\n",
    "        depthmax = random.uniform(depthmin, 0)  # depthmax will be less than or equal to depthmin\n",
    "        item = random.choice(items)\n",
    "        hard_perc = random.uniform(0, 100)  # Percentage of hard growth\n",
    "        hard_mm = random.uniform(0, 60)  # Thickness of hard growth in mm\n",
    "        soft_perc = random.uniform(0, 100)  # Percentage of soft growth\n",
    "        soft_mm = random.uniform(0, 40)  # Thickness of soft growth in mm\n",
    "        recommended_cleaning_method = random.choice(cleaning_methods)\n",
    "\n",
    "        synthetic_data.append({\n",
    "            \"platform\": structure_type,\n",
    "            \"year\": year,\n",
    "            \"depthmin\": depthmin,\n",
    "            \"depthmax\": depthmax,\n",
    "            \"item\": item,\n",
    "            \"hardPerc\": hard_perc,\n",
    "            \"hardmm\": hard_mm,\n",
    "            \"softPerc\": soft_perc,\n",
    "            \"softmm\": soft_mm,\n",
    "            \"Recommended_Cleaning_Method\": recommended_cleaning_method\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(synthetic_data)\n",
    "\n",
    "# Generate a synthetic dataset with 10 entries\n",
    "synthetic_dataset = generate_synthetic_data(10)\n",
    "\n",
    "# Output the synthetic dataset to a CSV file\n",
    "synthetic_dataset_path = '/mnt/data/synthetic_dataset.csv'\n",
    "synthetic_dataset.to_csv(synthetic_dataset_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/workspaces/UnderWater-Decision/data/excel/L10-BB.csv', '/workspaces/UnderWater-Decision/data/excel/D15-A.csv', '/workspaces/UnderWater-Decision/data/excel/L10-L.csv', '/workspaces/UnderWater-Decision/data/excel/L10-AC.csv', '/workspaces/UnderWater-Decision/data/excel/L10-AR.csv', '/workspaces/UnderWater-Decision/data/excel/L10-D.csv', '/workspaces/UnderWater-Decision/data/excel/K12-C.csv', '/workspaces/UnderWater-Decision/data/excel/Sheet9.csv', '/workspaces/UnderWater-Decision/data/excel/L10-E.csv', '/workspaces/UnderWater-Decision/data/excel/L5-A.csv', '/workspaces/UnderWater-Decision/data/excel/K12-G.csv', '/workspaces/UnderWater-Decision/data/excel/F3-1A.csv', '/workspaces/UnderWater-Decision/data/excel/G16-B.csv', '/workspaces/UnderWater-Decision/data/excel/L10-M.csv', '/workspaces/UnderWater-Decision/data/excel/K12-BP.csv', '/workspaces/UnderWater-Decision/data/excel/L10-AD.csv', '/workspaces/UnderWater-Decision/data/excel/L10-EE.csv', '/workspaces/UnderWater-Decision/data/excel/L10-B.csv', '/workspaces/UnderWater-Decision/data/excel/Location.csv', '/workspaces/UnderWater-Decision/data/excel/K9-B.csv', '/workspaces/UnderWater-Decision/data/excel/K12-D.csv', '/workspaces/UnderWater-Decision/data/excel/All.csv', '/workspaces/UnderWater-Decision/data/excel/K12-K.csv', '/workspaces/UnderWater-Decision/data/excel/E17-A.csv', '/workspaces/UnderWater-Decision/data/excel/G17-A.csv', '/workspaces/UnderWater-Decision/data/excel/Q13-A.csv', '/workspaces/UnderWater-Decision/data/excel/K12-CC.csv', '/workspaces/UnderWater-Decision/data/excel/G14-A.csv', '/workspaces/UnderWater-Decision/data/excel/L10-C.csv', '/workspaces/UnderWater-Decision/data/excel/K2-A.csv', '/workspaces/UnderWater-Decision/data/excel/L5-D.csv', '/workspaces/UnderWater-Decision/data/excel/L10-AP.csv', '/workspaces/UnderWater-Decision/data/excel/D18-A.csv', '/workspaces/UnderWater-Decision/data/excel/L10-F.csv', '/workspaces/UnderWater-Decision/data/excel/Explanation.csv', '/workspaces/UnderWater-Decision/data/excel/G17-AP.csv', '/workspaces/UnderWater-Decision/data/excel/L10-G.csv', '/workspaces/UnderWater-Decision/data/excel/K9-A.csv', '/workspaces/UnderWater-Decision/data/excel/K12-A.csv', '/workspaces/UnderWater-Decision/data/excel/K12-BD.csv', '/workspaces/UnderWater-Decision/data/excel/G14-B.csv', '/workspaces/UnderWater-Decision/data/excel/G16-A.csv', '/workspaces/UnderWater-Decision/data/excel/K9-C.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def export_excel_sheets_to_csv(excel_path, output_dir):\n",
    "    # Read the Excel file to get the sheet names\n",
    "    xls = pd.ExcelFile(excel_path)\n",
    "\n",
    "    # Create directory for CSV files if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Export each sheet to a separate CSV file\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        # Read the sheet into a pandas dataframe\n",
    "        df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "        \n",
    "        # Define the CSV file path\n",
    "        csv_file_path = os.path.join(output_dir, f'{sheet_name}.csv')\n",
    "        \n",
    "        # Save the dataframe to CSV\n",
    "        df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "    # Return a list of paths to the exported CSV files\n",
    "    return [os.path.join(output_dir, file) for file in os.listdir(output_dir)]\n",
    "\n",
    "# Usage\n",
    "excel_file_path = '/workspaces/UnderWater-Decision/data/mf-data.xlsx'  # Replace with your Excel file path\n",
    "csv_directory = '/workspaces/UnderWater-Decision/data/excel'   # Replace with your desired output directory path\n",
    "exported_files_paths = export_excel_sheets_to_csv(excel_file_path, csv_directory)\n",
    "\n",
    "# Print the paths to the exported CSV files\n",
    "print(exported_files_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path = '/workspaces/UnderWater-Decision/data/excel/All.csv'\n",
    "real_data_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "\n",
    "# Define thresholds for fouling characteristics\n",
    "HARD_PERCENTAGE_THRESHOLD = 50\n",
    "HARD_THICKNESS_THRESHOLD = 30\n",
    "SOFT_PERCENTAGE_THRESHOLD = 50\n",
    "SOFT_THICKNESS_THRESHOLD = 50\n",
    "\n",
    "# Constants for depth thresholds and coverage percentages\n",
    "SHALLOW_DEPTH_UPPER_BOUND = 0\n",
    "SHALLOW_DEPTH_LOWER_BOUND = -10\n",
    "MID_DEPTH_UPPER_BOUND = -25\n",
    "MID_DEPTH_LOWER_BOUND = -35\n",
    "DEEP_DEPTH_LOWER_BOUND = -40\n",
    "MAX_COVERAGE_PERCENTAGE = 90\n",
    "SHALLOW_DEPTH_COVERAGE_RANGE = (5, 11)\n",
    "DEEP_DEPTH_COVERAGE_RANGE = (70, 91)\n",
    "GENERIC_COVERAGE_RANGE = (5, 91)\n",
    "\n",
    "\n",
    "def determine_cleaning_method(hard_perc, hard_mm, soft_perc, soft_mm):\n",
    "    \"\"\"\n",
    "    Determines the cleaning method based on the fouling characteristics.\n",
    "    \n",
    "    Parameters:\n",
    "    - hard_perc (int): Percentage of hard fouling.\n",
    "    - hard_mm (int): Thickness of hard fouling in millimeters.\n",
    "    - soft_perc (int): Percentage of soft fouling.\n",
    "    - soft_mm (int): Thickness of soft fouling in millimeters.\n",
    "\n",
    "    Returns:\n",
    "    - str: The recommended cleaning method.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Mechanical cleaning for severe hard fouling\n",
    "    if hard_perc >= 75 or hard_mm >= 50:\n",
    "        return 'Mechanical cleaning methods'\n",
    "\n",
    "    # High-pressure water jetting for severe soft fouling\n",
    "    if soft_perc >= 75 or soft_mm >= SOFT_THICKNESS_THRESHOLD:\n",
    "        return 'High-pressure water jetting'\n",
    "\n",
    "    # Cavitation water jetting for significant hard and soft fouling\n",
    "    if hard_perc >= HARD_PERCENTAGE_THRESHOLD and soft_perc >= SOFT_PERCENTAGE_THRESHOLD:\n",
    "        return 'Cavitation water jetting'\n",
    "\n",
    "    # Ultrasonic cleaning for minor hard fouling without significant thickness\n",
    "    if hard_perc < HARD_PERCENTAGE_THRESHOLD and hard_mm < HARD_THICKNESS_THRESHOLD:\n",
    "        return 'Ultrasonic cleaning'\n",
    "\n",
    "    # Laser cleaning as a default for other cases\n",
    "    return 'Laser cleaning'\n",
    "\n",
    "\n",
    "def area_coverage_by_fouling_and_depth(hard_perc: int, soft_perc: int, depth: int) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the area coverage by fouling at a given depth.\n",
    "\n",
    "    Args:\n",
    "    hard_perc (int): The percentage of hard fouling.\n",
    "    soft_perc (int): The percentage of soft fouling.\n",
    "    depth (int): The depth in meters (negative for below sea level).\n",
    "\n",
    "    Returns:\n",
    "    int: The fouling coverage percentage.\n",
    "    \"\"\"\n",
    "    # Handle shallow depths with less coverage due to wave action and cleaning\n",
    "    if SHALLOW_DEPTH_LOWER_BOUND <= depth <= SHALLOW_DEPTH_UPPER_BOUND:\n",
    "        return random.randint(*SHALLOW_DEPTH_COVERAGE_RANGE)\n",
    "\n",
    "    # Handle mid-range depths with the highest fouling thickness\n",
    "    if MID_DEPTH_LOWER_BOUND <= depth <= MID_DEPTH_UPPER_BOUND:\n",
    "        return min(max(hard_perc, soft_perc), MAX_COVERAGE_PERCENTAGE)\n",
    "\n",
    "    # Handle deeper layers with higher coverage, but less than the peak range\n",
    "    if depth >= DEEP_DEPTH_LOWER_BOUND:\n",
    "        return random.randint(*DEEP_DEPTH_COVERAGE_RANGE)\n",
    "\n",
    "    # Default case for other depths\n",
    "    return min(max(hard_perc, soft_perc), random.randint(*GENERIC_COVERAGE_RANGE))\n",
    "\n",
    "    \n",
    "# Modify the synthetic data generation function to ensure all growth values are integers\n",
    "def generate_synthetic_data(real_df, num_entries):\n",
    "    synthetic_data = []\n",
    "    platforms = real_df['platform'].unique()\n",
    "    years = real_df['year'].unique()\n",
    "    depthmins = real_df['depthmin'].dropna().astype(int).unique()\n",
    "    depthmaxs = real_df['depthmax'].dropna().astype(int).unique()\n",
    "    items = real_df['Item'].unique()\n",
    "\n",
    "    for _ in range(num_entries):\n",
    "        platform = np.random.choice(platforms)\n",
    "        year = int(np.random.choice(years))\n",
    "        depthmin = np.random.choice(depthmins)\n",
    "        depthmax = np.random.choice(depthmaxs)\n",
    "        item = np.random.choice(items)\n",
    "        hard_perc = np.random.randint(0, 101)\n",
    "        hard_mm = np.random.randint(0, int(real_df['hardmm'].dropna().max()) + 1)\n",
    "        soft_perc = np.random.randint(0, 101 - hard_perc)\n",
    "        soft_mm = np.random.randint(0, int(real_df['softmm'].dropna().max()) + 1)\n",
    "        # Use the average depth for coverage calculation\n",
    "        avg_depth = (depthmin + depthmax) // 2\n",
    "        total_area_coverage = area_coverage_by_fouling_and_depth(hard_perc, soft_perc, avg_depth)\n",
    "        \n",
    "        cleaning_method = determine_cleaning_method(hard_perc, hard_mm, soft_perc, soft_mm)\n",
    "\n",
    "        synthetic_data.append({\n",
    "            \"platform\": platform,\n",
    "            \"year\": year,\n",
    "            \"depthmin\": depthmin,\n",
    "            \"depthmax\": depthmax,\n",
    "            \"item\": item,\n",
    "            \"hardPerc\": hard_perc,\n",
    "            \"hardmm\": hard_mm,\n",
    "            \"softPerc\": soft_perc,\n",
    "            \"softmm\": soft_mm,\n",
    "            \"Total_Area_Coverage\": total_area_coverage,\n",
    "            \"Recommended_Cleaning_Method\": cleaning_method\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(synthetic_data)\n",
    "\n",
    "# Generate a synthetic dataset with 10 entries based on the real CSV data\n",
    "synthetic_dataset = generate_synthetic_data(real_data_df, 1000)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "synthetic_dataset_path = '/workspaces/UnderWater-Decision/data/synthetic_dataset2.csv'\n",
    "synthetic_dataset.to_csv(synthetic_dataset_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uw-decision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
